{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAyKlyGxHT9"
      },
      "source": [
        "# PyTorch Tutorial & Homework - Neural Networks\n",
        "Prof. Lim Kwan Hui and Prof. Ezekiel Soremekun, with many thanks to Prof. Dorien Herremans for the initial version and Nelson Lui for the base text.\n",
        "\n",
        "Homework questions are at the end of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZnM_yfCBz6P"
      },
      "source": [
        "**To edit the notebook**:\n",
        "\n",
        "There are two ways to edit the notebook.\n",
        "\n",
        "You can either open it in the \"playground\", where you can change and run cells. After closing the tab, your changes will be lost. To do so, press \"File\" > \"Open in playground\".\n",
        "\n",
        "Alternatively, you can make a copy of this notebook to your own Google Drive account through \"File\" > \"Save a copy in Drive...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK-sJIRG6BLp"
      },
      "source": [
        "**Activating the GPU on Colab**:\n",
        "\n",
        "Colab now gives you 12 hours of free GPU time (before you have to request a new node).\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgs23T03xBK7"
      },
      "source": [
        "# Setting up the notebook on colab\n",
        "\n",
        "Let's check if we are using the GPU environment and cuda is installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "kig3C9d9D-kA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version:\n",
            "2.2.1\n",
            "GPU Detected:\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# Import PyTorch and other libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"PyTorch version:\")\n",
        "print(torch.__version__)\n",
        "print(\"GPU Detected:\")\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "#defining a shortcut function for later:\n",
        "import os\n",
        "using_GPU = os.path.exists('/opt/bin/nvidia-smi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE8RE8n5cUmJ"
      },
      "source": [
        "# Computation Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I00992FzWysI"
      },
      "source": [
        "A computation graph is simply a way to define a sequence of operations to go from input to model output.\n",
        "\n",
        "You can think of the nodes in the graph as representing operations, and the edges in the graph represent tensors going in and out.\n",
        "\n",
        "For example, say we wanted to build a linear regression model. This has the form $\\hat y = Wx + b$.\n",
        "\n",
        "In this equation, $x$ is our input, $W$ is a learned weight matrix, $b$ is a learned bias, and $\\hat y$ is the predicted output.\n",
        "\n",
        "As a computation graph, this looks like:\n",
        "\n",
        "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)\n",
        "\n",
        "When implementing deep learning models, you're basically designing and specifying computation graphs. It's a bit like playing with Legos in that you're stringing together a bunch of blocks (the operations) to achieve a final desired output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d2nO7nFW6vj"
      },
      "source": [
        "# The building blocks of deep learning models\n",
        "\n",
        "`torch.nn` makes it easy to build neural nets by providing functions for specifying arbitrary computation graphs and abstractions for putting them all together. We'll start by covering a few classes in the `torch.nn` module that form basic building blocks of many deep learning applications.\n",
        "\n",
        "The classes below are all callable, so you can use them with `outputs = YourDeepLearningBlock(its_inputs)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_jzL24p4YHeT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UAYmVadXZzM"
      },
      "source": [
        "## Linear Layers (Affine Transforms)\n",
        "\n",
        "A linear layer (also known as an affine transform) defines a function:\n",
        "\n",
        "$$f(x) = Wx + b$$\n",
        "\n",
        "This linear transform is a core part of deep learning. $W$ and $b$ are the parameters of this layer, where $W$ is a learned weight matrix and $b$ is a learned bias vector.\n",
        "\n",
        "`nn.Linear()` takes two construction parameters: the dimensionality of the input and the dimensionality of the desired output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vGzhJV4rYEI6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2427,  0.0437,  0.6581],\n",
            "        [-0.4578, -0.7820,  0.1781]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Create a Linear layer. Input should have 5 dimensions, output will have 3.\n",
        "lin = nn.Linear(5, 3)\n",
        "# Data is a matrix of shape (2, 5). Can we use the linear layer on it?\n",
        "data = torch.randn(2, 5)\n",
        "\n",
        "# Yes! Running the data matrix through the layer outputs shape (2, 3).\n",
        "print(lin(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "qK_mOB6oZWZ8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.6175, -0.1103, -0.4453],\n",
            "         [-0.5570,  1.1604,  0.7831],\n",
            "         [-0.0775,  1.1725,  0.5173],\n",
            "         [ 0.1109,  0.2320,  0.5300]],\n",
            "\n",
            "        [[-0.3850,  0.2671,  0.4981],\n",
            "         [ 0.0855,  0.2707,  0.5223],\n",
            "         [-0.1584,  0.5427, -0.1146],\n",
            "         [ 0.0279,  0.1225,  0.2458]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# What about a matrix of shape (2, 4, 5)?\n",
        "data = torch.randn(2, 4, 5)\n",
        "# This works as well! As long as the last dimension is the specified\n",
        "# input dimension to the Linear layer, you're good.\n",
        "# Output shape: (2, 4, 3)\n",
        "print(lin(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qTv_DwVnZywZ"
      },
      "outputs": [],
      "source": [
        "# But (5, 2) is an incompatible shape (uncomment and run to see error)\n",
        "data = torch.randn(5, 2)\n",
        "#print(lin(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Q07tNHV4aGhw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1913,  0.8792,  0.4457],\n",
            "        [ 0.2834,  1.1513,  0.1965]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# But we can transpose it using t()!\n",
        "# Now its shape (2, 5) and all is fine.\n",
        "print(lin(data.t()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ObbNWGsaR60"
      },
      "source": [
        "## Nonlinearities / Activation Functions\n",
        "\n",
        "Since composing linear transformations gives you a linear transformation, we don't gain any representational power by just chaining `Linear` layers.\n",
        "\n",
        "In deep learning, we add nonlinearities after our Linear transforms, which lets us build more powerful models.\n",
        "\n",
        "PyTorch comes with a veritable zoo of nonlinearities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "L0Of7Zc7bZoO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.8418, -0.8038,  1.2000],\n",
            "        [ 0.5772,  0.8405,  1.2129]])\n",
            "ReLU()\n",
            "tensor([[0.8418, 0.0000, 1.2000],\n",
            "        [0.5772, 0.8405, 1.2129]])\n",
            "Tanh()\n",
            "tensor([[ 0.6868, -0.6662,  0.8337],\n",
            "        [ 0.5207,  0.6861,  0.8375]])\n",
            "Sigmoid()\n",
            "tensor([[0.6988, 0.3092, 0.7685],\n",
            "        [0.6404, 0.6986, 0.7708]])\n"
          ]
        }
      ],
      "source": [
        "data = torch.randn(2, 3)\n",
        "print(data)\n",
        "\n",
        "# Nonlinearities are layers too!\n",
        "relu = nn.ReLU()\n",
        "print(relu)\n",
        "print(relu(data))\n",
        "\n",
        "tanh = nn.Tanh()\n",
        "print(tanh)\n",
        "print(tanh(data))\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "print(sigmoid)\n",
        "print(sigmoid(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6LvhuQ5cAA0"
      },
      "source": [
        "If you'd prefer to not create a class for the nonlinearity, you can also call it functionally as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8FC3JGzCa7nT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6563,  1.1358,  0.3073],\n",
            "        [-0.8320,  0.1175, -0.0789]])\n",
            "ReLu:\n",
            "tensor([[0.6563, 1.1358, 0.3073],\n",
            "        [0.0000, 0.1175, 0.0000]])\n",
            "tanh:\n",
            "tensor([[ 0.5759,  0.8130,  0.2980],\n",
            "        [-0.6815,  0.1170, -0.0788]])\n",
            "Sigmoid:\n",
            "tensor([[0.6584, 0.7569, 0.5762],\n",
            "        [0.3032, 0.5293, 0.4803]])\n"
          ]
        }
      ],
      "source": [
        "data = torch.randn(2, 3)\n",
        "print(data)\n",
        "\n",
        "# Nonlinearities can also be used functionally, with no need to create a class!\n",
        "print(\"ReLu:\")\n",
        "print(torch.relu(data))\n",
        "\n",
        "print(\"tanh:\")\n",
        "print(torch.tanh(data))\n",
        "\n",
        "print(\"Sigmoid:\")\n",
        "print(torch.sigmoid(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "609lvRL3cecR"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "Dropout is used to regularize our models by randomly setting some outputs to 0.\n",
        "\n",
        "This helps to prevent overfitting by encouraging the model to look beyond specific spurious patterns and find features that generalize.\n",
        "\n",
        "**Note that we should only apply dropout during training!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "62Tot0pNc8co"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.6832,  0.6947,  0.7942],\n",
            "        [ 1.0470, -0.8556, -0.6907]])\n",
            "Dropout(p=0.5, inplace=False)\n",
            "tensor([[-0.0000,  1.3894,  0.0000],\n",
            "        [ 0.0000, -1.7112, -1.3813]])\n",
            "Functional dropout, training=False\n",
            "tensor([[-0.6832,  0.6947,  0.7942],\n",
            "        [ 1.0470, -0.8556, -0.6907]])\n",
            "Functional dropout, training=True\n",
            "tensor([[-0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, -1.7112, -0.0000]])\n"
          ]
        }
      ],
      "source": [
        "data = torch.randn(2, 3)\n",
        "print(data)\n",
        "\n",
        "# Create a Dropout layer and call it on input\n",
        "# Here, the probability of zeroing an element is 0.5\n",
        "dropout = nn.Dropout(0.5)\n",
        "print(dropout)\n",
        "print(dropout(data))\n",
        "\n",
        "# Use dropout functionally, training=False by default so no change.\n",
        "print(\"Functional dropout, training=False\")\n",
        "print(F.dropout(data, 0.5, training=False))\n",
        "\n",
        "# Set training=True, so things are dropped out\n",
        "print(\"Functional dropout, training=True\")\n",
        "print(F.dropout(data, 0.5, training=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNnUJcrU2r2"
      },
      "source": [
        "# Structuring PyTorch models\n",
        "\n",
        "At the highest level, `nn.Module` defines what most would refer to as a \"model\". It's a convenient way for encapsulating the trainable parameters of a model or a component of your model, and subclassing this class gives you Python functions for moving your model to the GPU, saving it, loading it etc.\n",
        "\n",
        "When you're building your own model, you're going to subclass `nn.Module`. Critically, you also need to override the `__init__()` and `forward()` functions.\n",
        "\n",
        "*   In `__init__()`, you should take arguments that modify how the model runs (e.g. # of layers, # of hidden units, output sizes). You'll also set up most of the layers that you use in the forward pass here.\n",
        "*   In `forward()`, you define the \"forward pass\" of your model, or the operations needed to transform input to output. **You can use any of the Tensor operations in the forward pass.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goB1WdBQ3TKd"
      },
      "source": [
        "### Feed-forward neural net\n",
        "\n",
        "Back to the simple neural network we covered in the lecture, we can add some intermediate layers (called hidden layers), nonlinearities, and dropout for regularization. This is essentially a multi-layer feed forward neural net, and it's implementation as a module is outlined below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "dXe9xPrn3zSK"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "  # input_size: Dimensionality of input feature vector.\n",
        "  # num_classes: The number of classes in the classification problem.\n",
        "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
        "  # hidden_dim: The size of each of the hidden layers.\n",
        "  # dropout: The proportion of units to drop out after each layer.\n",
        "  def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
        "    # Always call the superclass (nn.Module) constructor first!\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "\n",
        "    # Set up the hidden layers.\n",
        "    assert num_hidden > 0\n",
        "    # A special ModuleList to store our hidden layers.\n",
        "    self.hidden_layers = nn.ModuleList([])\n",
        "    # First hidden layer maps from input_size -> num_hidden.\n",
        "    self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
        "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
        "    # Note that they can map to any dimensionality --- as long as the final\n",
        "    # output is a distribution over your classes!\n",
        "    for i in range(num_hidden - 1):\n",
        "      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "\n",
        "    # Set up the dropout layer.\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Set up the final transform to a distribution over classes.\n",
        "    self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    # Set up the nonlinearity to use between layers.\n",
        "    self.nonlinearity = nn.ReLU()\n",
        "\n",
        "  # Forward's sole argument is the input.\n",
        "  # input is of shape (batch_size, input_size)\n",
        "  def forward(self, x):\n",
        "    # Apply the hidden layers, nonlinearity, and dropout.\n",
        "    for hidden_layer in self.hidden_layers:\n",
        "      x = hidden_layer(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.nonlinearity(x)\n",
        "\n",
        "    # Output layer: project x to a distribution over classes.\n",
        "    out = self.output_projection(x)\n",
        "\n",
        "    # Softmax the out tensor to get a log-probability distribution\n",
        "    # over classes for each example.\n",
        "    out_distribution = F.log_softmax(out, dim=-1)\n",
        "    return out_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_oWVoYX6IJo"
      },
      "source": [
        "# Training PyTorch models: Losses and Optimizers\n",
        "\n",
        "By now, we've learned how to construct models in PyTorch. In this section, we'll go over how to calculate your model's loss and how to optimize the parameters to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-FcsSK266VJ"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "Intuitively, loss functions serve to tell your model how poorly it's doing --- the purpose of training is to adjust the weights of our model to minimize the loss.\n",
        "\n",
        "A loss function takes a true output $y$ and a model-predicted output $\\hat y$ and calculates the loss. If $y = \\hat y$, our model produced the correct output and thus our loss is 0. The further our predicted $\\hat y$ from the true $y$, the higher our loss is.\n",
        "\n",
        "PyTorch comes with a large collection of loss functions. The most commonly used loss for classification is negative log likelihood (`nn.NLLLoss` or the very related `nn.CrossEntropyLoss`). The difference between `nn.NLLLoss` and `nn.CrossEntropyLoss` for classification problems is that `nn.NLLLoss` expects the output to be log-softmax normalized, which is easy to do with the `nn.LogSoftmax` layer. On the other hand `nn.CrossEntropyLoss`, automatically applies the log-softmax --- you can think of it as `nn.LogSoftmax` + `nn.NLLLoss`. Which to use depends on whether you want to add the extra `nn.LogSoftmax` to your model's `forward()`.\n",
        "\n",
        "A common loss used for regression problems is the mean squared error (`nn.MSELoss`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfdPEetZARPk"
      },
      "source": [
        "Here's a usage example of the `CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "jHP6WENd9ql4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CrossEntropyLoss averaged across all 3 batch elements:\n",
            "tensor(1.1827, grad_fn=<NllLossBackward0>)\n",
            "Gradients of model_output\n",
            "tensor([[ 0.0500, -0.2170,  0.0714,  0.0956],\n",
            "        [-0.2491,  0.0745,  0.0689,  0.1057],\n",
            "        [ 0.0734,  0.0617,  0.0894, -0.2245]])\n"
          ]
        }
      ],
      "source": [
        "# 3 examples, unnormalized scores over 4 classes.\n",
        "model_output = torch.rand(3, 4, requires_grad = True)\n",
        "\n",
        "# The correct labels.\n",
        "targets = torch.LongTensor([1, 0, 3])\n",
        "\n",
        "# CrossEntropyLoss\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "# Loss, averaged across all 3 batch elements.\n",
        "# Can call this functionally: avg_loss = F.cross_entropy(model_output, targets)\n",
        "avg_loss = cross_entropy(model_output, targets)\n",
        "print(\"CrossEntropyLoss averaged across all 3 batch elements:\")\n",
        "print(avg_loss)\n",
        "\n",
        "# Backpropagate wrt avg_loss\n",
        "avg_loss.backward()\n",
        "# Print out the gradients of model_output\n",
        "print(\"Gradients of model_output\")\n",
        "print(model_output.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALpI3evA2H-"
      },
      "source": [
        "And here's a snippet showing that `LogSoftmax` + `NLLLoss` is the same as `CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "lLB3icyzA1bv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Negative-Log Likelihood averaged across all 3 batch elements:\n",
            "tensor(1.1827, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "nll = nn.NLLLoss()\n",
        "log_softmax_model_output = F.log_softmax(model_output, dim=-1)\n",
        "# Loss, averaged across all 3 batch elements.\n",
        "# Can call this functionally: avg_loss = F.nll_loss(model_output, targets)\n",
        "avg_loss = nll(log_softmax_model_output, targets)\n",
        "print(\"Negative-Log Likelihood averaged across all 3 batch elements:\")\n",
        "print(avg_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C4np6TBEToZ"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "Now that we can calculate the loss and backpropagate through our model (with `.backward()`), we can update the weights and try to reduce the loss!\n",
        "\n",
        "PyTorch includes a variety of optimizers that do exactly this, from the standard SGD to more recent techniques like Adam and RMSProp.\n",
        "\n",
        "At construction, PyTorch parameters take the parameters to optimize. When we run an input through our model, calculate the loss, and backpropagate, the gradients are automatically stored in the parameters (since they're all `Variables`). With these gradients, the optimizer can update the weights.\n",
        "\n",
        "Optimizers live in the `torch.optim` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "FSWQMLFIPqNt"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMNBP65mKvNY"
      },
      "source": [
        "To get the parameters of our model, we can just call `.parameters()` on a `Module`. Below, we create an instance of our previously-defined feed forward neural network and get its parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "e0s6-Y6LKurn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
            "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([50, 784]), torch.Size([50]), torch.Size([50, 50]), torch.Size([50]), torch.Size([10, 50]), torch.Size([10])]\n"
          ]
        }
      ],
      "source": [
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_hidden = 2\n",
        "hidden_dim = 50\n",
        "dropout = 0.2\n",
        "ffnn_clf = FeedForwardNN(input_size, num_classes, num_hidden,\n",
        "                         hidden_dim, dropout)\n",
        "print(ffnn_clf)\n",
        "\n",
        "parameters = ffnn_clf.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnL2QaEPcu5"
      },
      "source": [
        "Now to create an optimizer for this model, we construct a optimizer class and pass it the parameters of the model: stochastic gradient descend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_eUwD2d1PizL"
      },
      "outputs": [],
      "source": [
        "ffnn_optim = optim.SGD(ffnn_clf.parameters(), lr=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0LwAANLQUe8"
      },
      "source": [
        "Let's try using our optimizer to take a gradient update on our model! We'll generate a few random examples, and run them through our model (the forward pass)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "6SHk4heLQk3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted distribution over classes: \n",
            "tensor([[-2.4876, -2.2540, -2.3640, -2.4481, -2.1980, -2.4108, -2.2306, -2.2909,\n",
            "         -2.2048, -2.1918],\n",
            "        [-2.5271, -2.2726, -2.0243, -2.4829, -2.3892, -2.3375, -2.2993, -2.2415,\n",
            "         -2.1618, -2.3919],\n",
            "        [-2.3128, -2.1053, -2.1353, -2.3604, -2.4080, -2.3782, -2.5198, -2.2543,\n",
            "         -2.1861, -2.4521],\n",
            "        [-2.4563, -2.5450, -2.2291, -2.4671, -2.1358, -2.1607, -2.2910, -2.1736,\n",
            "         -2.2903, -2.3672],\n",
            "        [-2.4685, -2.1693, -2.1936, -2.3258, -2.3950, -2.2589, -2.4781, -2.2594,\n",
            "         -2.1467, -2.3969]], grad_fn=<LogSoftmaxBackward0>)\n",
            "Target Labels:\n",
            "tensor([0, 3, 9, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "# Make some fake data for our model.\n",
        "# 5 examples in the batch, each example has 784 features.\n",
        "sample_input = torch.randn(5, 784)\n",
        "# Multilabel classification, 10 possible classes.\n",
        "sample_labels = torch.LongTensor([0, 3, 9, 6, 2])\n",
        "\n",
        "# Run the sample_input through ffnn_clf to get a distribution\n",
        "# over our classes\n",
        "sample_predictions = ffnn_clf(sample_input)\n",
        "print(\"Predicted distribution over classes: \")\n",
        "print(sample_predictions)\n",
        "print(\"Target Labels:\")\n",
        "print(sample_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek8IxTtjS8vn"
      },
      "source": [
        "Now let's calculate the loss of our model on these examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "cmFz2EiBS_u7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average NLL Loss:\n",
            "tensor(2.3814, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "nll_loss = F.nll_loss(sample_predictions, sample_labels)\n",
        "print(\"Average NLL Loss:\")\n",
        "print(nll_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MVYKKL7TbiV"
      },
      "source": [
        "Let's print the gradients of one of the parameter matrices in our model, to ensure it's `None`. We haven't done backprop yet, so there shouldn't be any gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "iuffHd7WTj6Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(list(ffnn_clf.parameters())[0].grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-P0XKUvTRqg"
      },
      "source": [
        "Now we can backpropagate with respect to the loss to calculate the gradients for the parameters of our model with `.backward()`. It's also good practice to call `optimizer.zero_grad()` before `loss.backwards()`, which ensures that the gradients are reset to 0 before backprop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "tcXTmYqlTQ8h"
      },
      "outputs": [],
      "source": [
        "ffnn_optim.zero_grad()\n",
        "nll_loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxBJRkWjTvir"
      },
      "source": [
        "Let's check our gradients now..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PAhrOjDyTxls"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0101,  0.0054, -0.0388,  ...,  0.0399, -0.0127, -0.0016],\n",
            "        [ 0.0011, -0.0095,  0.0082,  ...,  0.0007,  0.0053,  0.0001],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-0.0043, -0.0101,  0.0061,  ...,  0.0061,  0.0093,  0.0054],\n",
            "        [-0.0212,  0.0111,  0.0089,  ..., -0.0173,  0.0111, -0.0100],\n",
            "        [-0.0203,  0.0033, -0.0018,  ...,  0.0081,  0.0147,  0.0094]])\n"
          ]
        }
      ],
      "source": [
        "print(list(ffnn_clf.parameters())[0].grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3vNshvNUELX"
      },
      "source": [
        "Now that we have gradients for each of our parameters, we can update them by using `optimizer.step()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "LBeoxxulUHj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Difference between weight matrix before and after update:\n",
            "tensor([[ 5.0517e-03,  2.6928e-03, -1.9398e-02,  ...,  1.9935e-02,\n",
            "         -6.3730e-03, -7.9811e-04],\n",
            "        [ 5.4368e-04, -4.7666e-03,  4.1083e-03,  ...,  3.6877e-04,\n",
            "          2.6550e-03,  5.4335e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        ...,\n",
            "        [-2.1362e-03, -5.0494e-03,  3.0563e-03,  ...,  3.0290e-03,\n",
            "          4.6596e-03,  2.7048e-03],\n",
            "        [-1.0605e-02,  5.5329e-03,  4.4616e-03,  ..., -8.6679e-03,\n",
            "          5.5312e-03, -4.9849e-03],\n",
            "        [-1.0153e-02,  1.6503e-03, -8.8810e-04,  ...,  4.0672e-03,\n",
            "          7.3690e-03,  4.7086e-03]])\n"
          ]
        }
      ],
      "source": [
        "# save the old value of the parameter for comparison later\n",
        "old_parameter = list(ffnn_clf.parameters())[0].data.clone()\n",
        "\n",
        "# Make a gradient update with our optimizer\n",
        "ffnn_optim.step()\n",
        "\n",
        "new_parameter = list(ffnn_clf.parameters())[0].data\n",
        "\n",
        "print(\"Difference between weight matrix before and after update:\")\n",
        "print(old_parameter - new_parameter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCKZg7yUZban"
      },
      "source": [
        "If you're familiar with the SGD update rule, you know that:\n",
        "\n",
        "$$ \\theta^{t+1} = \\theta^{t} - \\left( \\eta \\cdot \\nabla L \\left(\\theta^{t} \\right) \\right)$$\n",
        "\n",
        "Where $\\theta^{t}$ is the weight at time $t$, $\\eta$ is the learning rate, $\\nabla L(\\theta^{t})$ is the gradient. Since $\\eta = 0.5$, it makes perfect sense that the difference between the weight vectors printed above is exactly half of the gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6M0pgwgcFk0"
      },
      "source": [
        "# Example: Classification on FashionMNIST\n",
        "\n",
        "Let's use the `FeedForwardNN` model we built earlier to do a simple classification task! This example is meant to be an annotated walkthrough of how to build, train, and evaluate a model in PyTorch. We'll use the [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist), where we are tasked with classifying black and white images of clothes into 10 different classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vw-6V7XhHs3"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "We'll start by loading the data with `torchvision` --- knowing how to use torchvision isn't the point of this tutorial, so it's relatively unannotated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "WgbJYcQRiG2A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision==0.17 in /opt/homebrew/lib/python3.10/site-packages (0.17.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from torchvision==0.17) (1.26.4)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from torchvision==0.17) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision==0.17) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision==0.17) (10.2.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from torch==2.2.0->torchvision==0.17) (2024.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision==0.17) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision==0.17) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision==0.17) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->torchvision==0.17) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch==2.2.0->torchvision==0.17) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->torch==2.2.0->torchvision==0.17) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.17 #note: you can find compatible torch/torchvision versions here: https://github.com/pytorch/vision#installation\n",
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "train_dataset = FashionMNIST(root='./torchvision-data',\n",
        "                             train=True,\n",
        "                             transform=torchvision.transforms.ToTensor(),\n",
        "                             download=True)\n",
        "\n",
        "test_dataset = FashionMNIST(root='./torchvision-data', train=False,\n",
        "                            transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry0GX49nj2T5"
      },
      "source": [
        "`train_dataset` and `test_dataset` are both subclasses of PyTorch's `torch.utils.data.Dataset`. The main benefit of subclassing this abstract class is that we can use `torch.utils.data.DataLoader`s to handle batching our examples and iterating over them. We'll create `DataLoader`s for our datasets now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "diAuxKlRn8pZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data-related hyperparameters\n",
        "batch_size = 64\n",
        "\n",
        "# Set up a DataLoader for the training dataset.\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up a DataLoader for the test dataset.\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWx3AsQoixs"
      },
      "source": [
        "Let's take a look at what's inside our datasets. `torch.utils.data.Dataset`s are indexable, so we can easily peek inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "NS2eyMtSovq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
            "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
            "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
            "          0.0157, 0.0000, 0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
            "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0471, 0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
            "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
            "          0.3020, 0.5098, 0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
            "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
            "          0.5529, 0.3451, 0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
            "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
            "          0.4824, 0.7686, 0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
            "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
            "          0.8745, 0.9608, 0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
            "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
            "          0.8627, 0.9529, 0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
            "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
            "          0.8863, 0.7725, 0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
            "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
            "          0.9608, 0.4667, 0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
            "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
            "          0.8510, 0.8196, 0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
            "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
            "          0.8549, 1.0000, 0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
            "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
            "          0.8784, 0.9569, 0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
            "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
            "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
            "          0.9137, 0.9333, 0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
            "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
            "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
            "          0.8627, 0.9098, 0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
            "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
            "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
            "          0.8706, 0.8941, 0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
            "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
            "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
            "          0.8745, 0.8784, 0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
            "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
            "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
            "          0.8627, 0.8667, 0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
            "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
            "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
            "          0.7098, 0.8039, 0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
            "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
            "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
            "          0.6549, 0.6941, 0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
            "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
            "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
            "          0.7529, 0.8471, 0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
            "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
            "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
            "          0.3882, 0.2275, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
            "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]]), 9)\n"
          ]
        }
      ],
      "source": [
        "# Print the first training example\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WHfersco6pG"
      },
      "source": [
        "From this output, we can see the dataset elements are tuple of `(data_tensor, label)`. `data_tensor` is a `FloatTensor` of shape `(1, 28, 28)` (since the image is 28x28), and `label` is an integer from 0 to 9 (since there are 10 classes in the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlass4LfpKyL"
      },
      "source": [
        "Let's similarly look at what the `DataLoader` produces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "fYxZWUHFpQCD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.1373, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
              " tensor([3, 8, 7, 1, 7, 0, 6, 0, 7, 6, 5, 7, 0, 7, 1, 3, 8, 7, 9, 6, 5, 3, 3, 1,\n",
              "         0, 2, 4, 7, 5, 3, 5, 6, 0, 7, 6, 8, 6, 6, 8, 9, 8, 1, 4, 6, 4, 1, 6, 1,\n",
              "         1, 4, 8, 1, 8, 8, 5, 2, 7, 0, 7, 4, 6, 5, 8, 6])]"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(train_dataloader)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9suOk1ppR7"
      },
      "source": [
        "As we can see, the `DataLoader` groups examples into batches of size `batch_size` (64 by default in the code above). Thus, the shape of the returned tensor is `(64, 1, 28, 28)`, since we essentially stacked `batch_size` examples together. Similarly, `labels` is now a `LongTensor` of size `batch_size`.\n",
        "\n",
        "Note that the label for a single example was a Python `int` --- the dataloader automatically grouped them into a `LongTensor` of the appropriate size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOsLTW_hkneW"
      },
      "source": [
        "## Building our model\n",
        "\n",
        "Now we can construct a `FeedForwardNN` instance that we'll train. Each FashionMNIST example is `28x28`, so we get it as a Tensor of shape `(28, 28)`.\n",
        "\n",
        "We'll flatten out each example to a vector of size `(784,)` for compatibility with our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "QVfwXTtdlCH1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters of our model.\n",
        "num_hidden = 2\n",
        "hidden_dim = 512\n",
        "dropout = 0.2\n",
        "\n",
        "fashionmnist_ffnn_clf = FeedForwardNN(input_size=784, num_classes=10,\n",
        "                                      num_hidden=num_hidden,\n",
        "                                      hidden_dim=hidden_dim, dropout=dropout)\n",
        "print(fashionmnist_ffnn_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufdJjQllZ9r"
      },
      "source": [
        "If we're using a GPU, we'll move the model to the GPU which should speed up training. We do this with the same `.cuda()` method we used for Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "PDzmZsi3lmea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model on GPU?:\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "if using_GPU:\n",
        "  fashionmnist_ffnn_clf = fashionmnist_ffnn_clf.cuda()\n",
        "\n",
        "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
        "print(\"Model on GPU?:\")\n",
        "print(next(fashionmnist_ffnn_clf.parameters()).is_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-zvR2a7mNBf"
      },
      "source": [
        "## Construct other classes we need for training: loss and optimizer\n",
        "\n",
        "Now, we'll set up a criterion for calculating the loss and an Optimizer for updating our parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "QEIHPun3mapF"
      },
      "outputs": [],
      "source": [
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "ffnn_optimizer = optim.SGD(fashionmnist_ffnn_clf.parameters(),\n",
        "                           lr=lr, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLpxMEcPnBSS"
      },
      "source": [
        "## Train the model!\n",
        "\n",
        "Now, we'll implement the procedure to train the model --- this is typically called the \"train loop\" since we loop over our batches, performing the forward pass, calculating a loss, backpropping, and then updating our parameters. This is the bulk of the code necessary to train the model.\n",
        "\n",
        "This block looks pretty long, but that's mostly because of the comments :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "jFWe_FxbnTTm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/p0/6sw494x1303f8w2k35bxtgk80000gn/T/ipykernel_85884/1347851882.py:63: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "/var/folders/p0/6sw494x1303f8w2k35bxtgk80000gn/T/ipykernel_85884/1347851882.py:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.5992898344993591. Test Accuracy 78.23999786376953.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5732710957527161. Test Accuracy 80.72000122070312.\n",
            "Iteration 1500. Test Loss 0.554969847202301. Test Accuracy 79.45999908447266.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.5411611199378967. Test Accuracy 80.9800033569336.\n",
            "Iteration 2500. Test Loss 0.5333136916160583. Test Accuracy 80.20999908447266.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.4768065810203552. Test Accuracy 82.38999938964844.\n",
            "Iteration 3500. Test Loss 0.5478038787841797. Test Accuracy 79.43000030517578.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.5184205174446106. Test Accuracy 81.91000366210938.\n",
            "Iteration 4500. Test Loss 0.4889791011810303. Test Accuracy 83.20999908447266.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.47661474347114563. Test Accuracy 83.1500015258789.\n",
            "Iteration 5500. Test Loss 0.5552670955657959. Test Accuracy 81.36000061035156.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5004670023918152. Test Accuracy 81.8499984741211.\n",
            "Iteration 6500. Test Loss 0.5110759139060974. Test Accuracy 83.25.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.4983198642730713. Test Accuracy 80.70999908447266.\n",
            "Iteration 7500. Test Loss 0.508216381072998. Test Accuracy 80.69999694824219.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.5020410418510437. Test Accuracy 80.52999877929688.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4838383197784424. Test Accuracy 83.13999938964844.\n",
            "Iteration 9000. Test Loss 0.4810347259044647. Test Accuracy 83.37999725341797.\n"
          ]
        }
      ],
      "source": [
        "# Number of epochs (passes through the dataset) to train the model for.\n",
        "num_epochs = 10\n",
        "\n",
        "# A counter for the number of gradient updates we've performed.\n",
        "num_iter = 0\n",
        "\n",
        "# Iterate `num_epochs` times.\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Starting epoch {}\".format(epoch + 1))\n",
        "  # Iterate over the train_dataloader, unpacking the images and labels\n",
        "  for (images, labels) in train_dataloader:\n",
        "    # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784), since\n",
        "    # that's what our model expects. Remember that -1 does shape inference!\n",
        "    reshaped_images = images.view(-1, 784)\n",
        "\n",
        "    # Wrap reshaped_images and labels in Variables,\n",
        "    # since we want to calculate gradients and backprop.\n",
        "    reshaped_images = Variable(reshaped_images)\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
        "    if using_GPU:\n",
        "      reshaped_images = reshaped_images.cuda()\n",
        "      labels = labels.cuda()\n",
        "\n",
        "    # Run the forward pass through the model to get predicted log distribution.\n",
        "    # predicted shape: (batch_size, 10) (since there are 10 classes)\n",
        "    predicted = fashionmnist_ffnn_clf(reshaped_images)\n",
        "\n",
        "    # Calculate the loss\n",
        "    batch_loss = nll_criterion(predicted, labels)\n",
        "\n",
        "    # Clear the gradients as we prepare to backprop.\n",
        "    ffnn_optimizer.zero_grad()\n",
        "\n",
        "    # Backprop (backward pass), which calculates gradients.\n",
        "    batch_loss.backward()\n",
        "\n",
        "    # Take a gradient step to update parameters.\n",
        "    ffnn_optimizer.step()\n",
        "\n",
        "    # Increment gradient update counter.\n",
        "    num_iter += 1\n",
        "\n",
        "    # Calculate test set loss and accuracy every 500 gradient updates\n",
        "    # It's standard to have this as a separate evaluate function, but\n",
        "    # we'll place it inline for didactic purposes.\n",
        "    if num_iter % 500 == 0:\n",
        "      # Set model to eval mode, which turns off dropout.\n",
        "      fashionmnist_ffnn_clf.eval()\n",
        "      # Counters for the num of examples we get right / total num of examples.\n",
        "      num_correct = 0\n",
        "      total_examples = 0\n",
        "      total_test_loss = 0\n",
        "\n",
        "      # Iterate over the test dataloader\n",
        "      for (test_images, test_labels) in test_dataloader:\n",
        "        # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784) again\n",
        "        reshaped_test_images = test_images.view(-1, 784)\n",
        "\n",
        "        # Wrap test data in Variable, like we did earlier.\n",
        "        # We set volatile=True bc we don't need history; speeds up inference.\n",
        "        reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
        "        test_labels = Variable(test_labels, volatile=True)\n",
        "\n",
        "        # If we're using the GPU, move tensors to the GPU.\n",
        "        if using_GPU:\n",
        "          reshaped_test_images = reshaped_test_images.cuda()\n",
        "          test_labels = test_labels.cuda()\n",
        "\n",
        "        # Run the forward pass to get predicted distribution.\n",
        "        predicted = fashionmnist_ffnn_clf(reshaped_test_images)\n",
        "\n",
        "        # Calculate loss for this test batch. This is averaged, so multiply\n",
        "        # by the number of examples in batch to get a total.\n",
        "        total_test_loss += nll_criterion(\n",
        "            predicted, test_labels).data * test_labels.size(0)\n",
        "\n",
        "        # Get predicted labels (argmax)\n",
        "        # We need predicted.data since predicted is a Variable, and torch.max\n",
        "        # expects a Tensor as input. .data extracts Tensor underlying Variable.\n",
        "        _, predicted_labels = torch.max(predicted.data, 1)\n",
        "\n",
        "        # Count the number of examples in this batch\n",
        "        total_examples += test_labels.size(0)\n",
        "\n",
        "        # Count the total number of correctly predicted labels.\n",
        "        # predicted == labels generates a ByteTensor in indices where\n",
        "        # predicted and labels match, so we can sum to get the num correct.\n",
        "        num_correct += torch.sum(predicted_labels == test_labels.data)\n",
        "      accuracy = 100 * num_correct / total_examples\n",
        "      average_test_loss = total_test_loss / total_examples\n",
        "      print(\"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
        "          num_iter, average_test_loss, accuracy))\n",
        "      # Set the model back to train mode, which activates dropout again.\n",
        "      fashionmnist_ffnn_clf.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XCqzzlc3_Zw"
      },
      "source": [
        "# Homework Exercises\n",
        "**Due: 7th Mar, 11:59pm**\n",
        "<br>\n",
        "<br>\n",
        "Based on the same FashionMNIST dataset, work on the following tasks below. Submit your homework as either: (i) an ipynb file with your results inside; or (ii) a python file and separate pdf discussing your results.\n",
        "\n",
        "(a) Develop a new feed-forward neural network that contains 3 hidden layers, with hidden layers 1, 2, 3 being of dimensions 512, 256, 128, respectively. Hidden layer 1 is the layer immediately after the input layer, while hidden layer 3 is the one just before the output layer.\n",
        "\n",
        "(b) Experiment with three different activation functions and two different optimizers. Report your results and discuss your findings.\n",
        "\n",
        "(c) Building upon Task b above, describe and implement two approaches to improve upon the best variation from Task b. Report your results and discuss your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part a:\n",
        "\n",
        "The following feed forward network is composed of three hidden layers where each layer is of the respective dimensions - 512, 256 and 128. \n",
        "I will be experimenting with three different activation functions namely, ReLU, tanh and Sigmoid since they are most commonly implemente along with one dropout function. After the implementation of each activation function, a dropout function will be implemented to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NewFeedForwardNet(\n",
            "  (layer1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (tanh): Tanh()\n",
            "  (sigmoid): Sigmoid()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the FeedForward Neural Network\n",
        "class NewFeedForwardNet(nn.Module):\n",
        "    def __init__(self, input_layer=784, output_layer=10, hidden_layer1=512, hidden_layer2=256, hidden_layer3=128, dropout=0.3):\n",
        "        super(NewFeedForwardNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_layer, hidden_layer1)\n",
        "        self.layer2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
        "        self.layer3 = nn.Linear(hidden_layer2, hidden_layer3)\n",
        "        self.output = nn.Linear(hidden_layer3, output_layer)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, activation='relu'):\n",
        "        x = x.view(x.shape[0], -1)  # Flatten input\n",
        "\n",
        "        if activation == 'relu':\n",
        "            activation_fn = self.relu\n",
        "        elif activation == 'tanh':\n",
        "            activation_fn = self.tanh\n",
        "        elif activation == 'sigmoid':\n",
        "            activation_fn = self.sigmoid\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "        x = activation_fn(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = activation_fn(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = activation_fn(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "# Instantiate the model\n",
        "new_model = NewFeedForwardNet()\n",
        "print(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the initial structural implementation of the layers is defined, I will proceed to train and evaluate the model using the train_and_evaluate function below. \n",
        "\n",
        "I will begin by ensuring the device in use where the function will be implemented using the CUDA GPU is available otherwise it will use the CPU resources. \n",
        "\n",
        "For the optimizers, I have chosen Adam and Stochastic Gradient Descent (SGD) since they pair well with the activation functions chose in the previous section. \n",
        "\n",
        "Adam: Initial learning rate = 0.001 \n",
        "SGD: Initial learning rate = 0.001, momentum = 0.9\n",
        "\n",
        "  optimizer.zero_grad() ensures the gradients are set to zero before each implementation so that they don't accumulate and affect the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with ReLU + Adam:\n",
            "Epoch 1: Loss = 557.9737, Training Accuracy = 0.7834\n",
            "Epoch 2: Loss = 400.5800, Training Accuracy = 0.8458\n",
            "Epoch 3: Loss = 363.2905, Training Accuracy = 0.8592\n",
            "Epoch 4: Loss = 345.3349, Training Accuracy = 0.8670\n",
            "Epoch 5: Loss = 331.3600, Training Accuracy = 0.8715\n",
            "Epoch 6: Loss = 318.8402, Training Accuracy = 0.8758\n",
            "Epoch 7: Loss = 310.0679, Training Accuracy = 0.8785\n",
            "Epoch 8: Loss = 302.0291, Training Accuracy = 0.8823\n",
            "Epoch 9: Loss = 294.8465, Training Accuracy = 0.8852\n",
            "Epoch 10: Loss = 287.6772, Training Accuracy = 0.8866\n",
            "\n",
            "Training with Tanh + Adam:\n",
            "Epoch 1: Loss = 521.5704, Training Accuracy = 0.7984\n",
            "Epoch 2: Loss = 414.3546, Training Accuracy = 0.8412\n",
            "Epoch 3: Loss = 386.7568, Training Accuracy = 0.8525\n",
            "Epoch 4: Loss = 369.2481, Training Accuracy = 0.8586\n",
            "Epoch 5: Loss = 356.1026, Training Accuracy = 0.8628\n",
            "Epoch 6: Loss = 348.8328, Training Accuracy = 0.8655\n",
            "Epoch 7: Loss = 339.0565, Training Accuracy = 0.8703\n",
            "Epoch 8: Loss = 335.7524, Training Accuracy = 0.8700\n",
            "Epoch 9: Loss = 327.7569, Training Accuracy = 0.8731\n",
            "Epoch 10: Loss = 324.8639, Training Accuracy = 0.8744\n",
            "\n",
            "Training with Sigmoid + SGD:\n",
            "Epoch 1: Loss = 2175.5396, Training Accuracy = 0.0987\n",
            "Epoch 2: Loss = 2170.9485, Training Accuracy = 0.1006\n",
            "Epoch 3: Loss = 2168.2480, Training Accuracy = 0.1008\n",
            "Epoch 4: Loss = 2167.3458, Training Accuracy = 0.0995\n",
            "Epoch 5: Loss = 2164.5840, Training Accuracy = 0.1019\n",
            "Epoch 6: Loss = 2164.3544, Training Accuracy = 0.1020\n",
            "Epoch 7: Loss = 2163.4535, Training Accuracy = 0.1011\n",
            "Epoch 8: Loss = 2162.8795, Training Accuracy = 0.1022\n",
            "Epoch 9: Loss = 2161.8652, Training Accuracy = 0.1036\n",
            "Epoch 10: Loss = 2160.6367, Training Accuracy = 0.1057\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(activation='relu', optimizer_type='adam', epochs=10):\n",
        "    model = NewFeedForwardNet()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer choice\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images, activation)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == labels).sum().item()\n",
        "        \n",
        "        train_accuracy = correct / len(train_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_accuracy:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run experiments\n",
        "print(\"Training with ReLU + Adam:\")\n",
        "model_relu_adam = train_and_evaluate('relu', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Tanh + Adam:\")\n",
        "model_tanh_adam = train_and_evaluate('tanh', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Sigmoid + SGD:\")\n",
        "model_sigmoid_sgd = train_and_evaluate('sigmoid', 'sgd')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inital test results show that ReLU+Adam had the highest accuracy of ~88% (0.8866), tanh+Adam had an accuracy of ~87% (0.8744) and the lowest accuracy of ~10% (0.1057) was Sigmoid+SGD.\n",
        "\n",
        "Hence, the nezxt iteration was aimed at improving the performance of Sigmoid+SGD by altering the parameters. In iteration 1 below, I increased the learning rate of SGD from 0.001 to 0.01 while keeping all other variables unchanged. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with ReLU + Adam:\n",
            "Epoch 1: Loss = 562.0038, Training Accuracy = 0.7818\n",
            "Epoch 2: Loss = 404.4496, Training Accuracy = 0.8447\n",
            "Epoch 3: Loss = 368.2507, Training Accuracy = 0.8597\n",
            "Epoch 4: Loss = 349.2219, Training Accuracy = 0.8653\n",
            "Epoch 5: Loss = 332.8848, Training Accuracy = 0.8712\n",
            "Epoch 6: Loss = 321.4450, Training Accuracy = 0.8757\n",
            "Epoch 7: Loss = 311.9161, Training Accuracy = 0.8792\n",
            "Epoch 8: Loss = 302.2629, Training Accuracy = 0.8822\n",
            "Epoch 9: Loss = 293.2805, Training Accuracy = 0.8860\n",
            "Epoch 10: Loss = 288.6774, Training Accuracy = 0.8875\n",
            "\n",
            "Training with Tanh + Adam:\n",
            "Epoch 1: Loss = 525.9455, Training Accuracy = 0.7984\n",
            "Epoch 2: Loss = 408.7132, Training Accuracy = 0.8427\n",
            "Epoch 3: Loss = 383.2703, Training Accuracy = 0.8528\n",
            "Epoch 4: Loss = 368.5491, Training Accuracy = 0.8595\n",
            "Epoch 5: Loss = 354.9660, Training Accuracy = 0.8630\n",
            "Epoch 6: Loss = 344.7906, Training Accuracy = 0.8656\n",
            "Epoch 7: Loss = 340.0693, Training Accuracy = 0.8701\n",
            "Epoch 8: Loss = 333.9088, Training Accuracy = 0.8713\n",
            "Epoch 9: Loss = 326.5610, Training Accuracy = 0.8736\n",
            "Epoch 10: Loss = 322.4648, Training Accuracy = 0.8761\n",
            "\n",
            "Training with Sigmoid + SGD:\n",
            "Epoch 1: Loss = 2172.1603, Training Accuracy = 0.1014\n",
            "Epoch 2: Loss = 2064.7089, Training Accuracy = 0.1444\n",
            "Epoch 3: Loss = 1459.8403, Training Accuracy = 0.3276\n",
            "Epoch 4: Loss = 1135.1532, Training Accuracy = 0.4957\n",
            "Epoch 5: Loss = 954.4430, Training Accuracy = 0.5903\n",
            "Epoch 6: Loss = 864.4299, Training Accuracy = 0.6353\n",
            "Epoch 7: Loss = 806.6361, Training Accuracy = 0.6681\n",
            "Epoch 8: Loss = 761.2710, Training Accuracy = 0.6942\n",
            "Epoch 9: Loss = 726.9156, Training Accuracy = 0.7094\n",
            "Epoch 10: Loss = 696.2551, Training Accuracy = 0.7252\n"
          ]
        }
      ],
      "source": [
        "#Iteration 1\n",
        "def train_and_evaluate(activation='relu', optimizer_type='adam', epochs=10):\n",
        "    model = NewFeedForwardNet()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer choice\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images, activation)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == labels).sum().item()\n",
        "        \n",
        "        train_accuracy = correct / len(train_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_accuracy:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run experiments\n",
        "print(\"Training with ReLU + Adam:\")\n",
        "model_relu_adam = train_and_evaluate('relu', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Tanh + Adam:\")\n",
        "model_tanh_adam = train_and_evaluate('tanh', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Sigmoid + SGD:\")\n",
        "model_sigmoid_sgd = train_and_evaluate('sigmoid', 'sgd')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results of iteration 1 shows an overall improvement in the accuracy of all models, especially the accuracy of Sigmoid+SGD which increased from ~10% (0.1057) in the initial implemented to 72% (0.7252)! in iteration 1. From this result we can infer that increasing the learning rate had a major impact in the accuracy of Sigmoid activation function with SGD as well as the loss which reduced drastically from ~2160 to ~696."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In iteration 2 below, I will increase the learning rate of SGD from 0.01 to 0.1 to observe if the accuracy improves and if the loss decreases further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with ReLU + Adam:\n",
            "Epoch 1: Loss = 559.5788, Training Accuracy = 0.7825\n",
            "Epoch 2: Loss = 400.5652, Training Accuracy = 0.8463\n",
            "Epoch 3: Loss = 366.4172, Training Accuracy = 0.8592\n",
            "Epoch 4: Loss = 347.7177, Training Accuracy = 0.8656\n",
            "Epoch 5: Loss = 330.4547, Training Accuracy = 0.8723\n",
            "Epoch 6: Loss = 318.9842, Training Accuracy = 0.8765\n",
            "Epoch 7: Loss = 309.1889, Training Accuracy = 0.8791\n",
            "Epoch 8: Loss = 303.9820, Training Accuracy = 0.8821\n",
            "Epoch 9: Loss = 294.1227, Training Accuracy = 0.8857\n",
            "Epoch 10: Loss = 285.1771, Training Accuracy = 0.8883\n",
            "\n",
            "Training with Tanh + Adam:\n",
            "Epoch 1: Loss = 522.2632, Training Accuracy = 0.8006\n",
            "Epoch 2: Loss = 410.7835, Training Accuracy = 0.8407\n",
            "Epoch 3: Loss = 378.8818, Training Accuracy = 0.8541\n",
            "Epoch 4: Loss = 364.3579, Training Accuracy = 0.8594\n",
            "Epoch 5: Loss = 355.6537, Training Accuracy = 0.8625\n",
            "Epoch 6: Loss = 346.2382, Training Accuracy = 0.8660\n",
            "Epoch 7: Loss = 336.9866, Training Accuracy = 0.8699\n",
            "Epoch 8: Loss = 331.8873, Training Accuracy = 0.8722\n",
            "Epoch 9: Loss = 327.5496, Training Accuracy = 0.8728\n",
            "Epoch 10: Loss = 322.7016, Training Accuracy = 0.8755\n",
            "\n",
            "Training with Sigmoid + SGD:\n",
            "Epoch 1: Loss = 1730.2515, Training Accuracy = 0.2579\n",
            "Epoch 2: Loss = 737.5067, Training Accuracy = 0.6953\n",
            "Epoch 3: Loss = 562.3509, Training Accuracy = 0.7821\n",
            "Epoch 4: Loss = 492.8695, Training Accuracy = 0.8137\n",
            "Epoch 5: Loss = 454.1392, Training Accuracy = 0.8274\n",
            "Epoch 6: Loss = 433.7624, Training Accuracy = 0.8362\n",
            "Epoch 7: Loss = 409.9790, Training Accuracy = 0.8436\n",
            "Epoch 8: Loss = 396.6315, Training Accuracy = 0.8492\n",
            "Epoch 9: Loss = 382.0853, Training Accuracy = 0.8538\n",
            "Epoch 10: Loss = 371.2543, Training Accuracy = 0.8572\n"
          ]
        }
      ],
      "source": [
        "#Iteration 2\n",
        "def train_and_evaluate(activation='relu', optimizer_type='adam', epochs=10):\n",
        "    model = NewFeedForwardNet()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer choice\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images, activation)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == labels).sum().item()\n",
        "        \n",
        "        train_accuracy = correct / len(train_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_accuracy:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run experiments\n",
        "print(\"Training with ReLU + Adam:\")\n",
        "model_relu_adam = train_and_evaluate('relu', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Tanh + Adam:\")\n",
        "model_tanh_adam = train_and_evaluate('tanh', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Sigmoid + SGD:\")\n",
        "model_sigmoid_sgd = train_and_evaluate('sigmoid', 'sgd')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results of iteration 2 shows a ~13% increase in the accuracy of Sigmoid+SGD from ~72% (0.7252) in iteration 1 to ~85% (0.8572) in iteration 2, as well as a drastic reduction in loss ~696 in iteration 1 to ~371 in iteration 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the final iteration, iteration 3 below, I will increase the learning rate of SGD from 0.1 to 1.0 while keeping all other variables unchanged to observe a possible improvement in accuracy and loss rates. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with ReLU + Adam:\n",
            "Epoch 1: Loss = 566.5738, Training Accuracy = 0.7777\n",
            "Epoch 2: Loss = 401.0009, Training Accuracy = 0.8465\n",
            "Epoch 3: Loss = 368.7477, Training Accuracy = 0.8567\n",
            "Epoch 4: Loss = 346.4264, Training Accuracy = 0.8675\n",
            "Epoch 5: Loss = 331.7427, Training Accuracy = 0.8708\n",
            "Epoch 6: Loss = 318.6099, Training Accuracy = 0.8769\n",
            "Epoch 7: Loss = 312.9610, Training Accuracy = 0.8787\n",
            "Epoch 8: Loss = 301.5828, Training Accuracy = 0.8831\n",
            "Epoch 9: Loss = 297.4196, Training Accuracy = 0.8829\n",
            "Epoch 10: Loss = 289.5726, Training Accuracy = 0.8874\n",
            "\n",
            "Training with Tanh + Adam:\n",
            "Epoch 1: Loss = 526.1339, Training Accuracy = 0.7991\n",
            "Epoch 2: Loss = 410.0683, Training Accuracy = 0.8452\n",
            "Epoch 3: Loss = 385.6389, Training Accuracy = 0.8517\n",
            "Epoch 4: Loss = 366.4562, Training Accuracy = 0.8579\n",
            "Epoch 5: Loss = 355.0481, Training Accuracy = 0.8626\n",
            "Epoch 6: Loss = 343.3016, Training Accuracy = 0.8673\n",
            "Epoch 7: Loss = 337.9778, Training Accuracy = 0.8693\n",
            "Epoch 8: Loss = 333.9000, Training Accuracy = 0.8705\n",
            "Epoch 9: Loss = 329.6200, Training Accuracy = 0.8742\n",
            "Epoch 10: Loss = 326.8136, Training Accuracy = 0.8725\n",
            "\n",
            "Training with Sigmoid + SGD:\n",
            "Epoch 1: Loss = 2198.2132, Training Accuracy = 0.0992\n",
            "Epoch 2: Loss = 2191.9640, Training Accuracy = 0.0995\n",
            "Epoch 3: Loss = 2193.3258, Training Accuracy = 0.1002\n",
            "Epoch 4: Loss = 2193.2055, Training Accuracy = 0.1006\n",
            "Epoch 5: Loss = 2193.1810, Training Accuracy = 0.0988\n",
            "Epoch 6: Loss = 2193.5408, Training Accuracy = 0.0992\n",
            "Epoch 7: Loss = 2194.3170, Training Accuracy = 0.0990\n",
            "Epoch 8: Loss = 2194.5910, Training Accuracy = 0.0968\n",
            "Epoch 9: Loss = 2193.6668, Training Accuracy = 0.1026\n",
            "Epoch 10: Loss = 2196.2161, Training Accuracy = 0.1003\n"
          ]
        }
      ],
      "source": [
        "#Iteration 3\n",
        "def train_and_evaluate(activation='relu', optimizer_type='adam', epochs=10):\n",
        "    model = NewFeedForwardNet()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=1.0, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer choice\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images, activation)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == labels).sum().item()\n",
        "        \n",
        "        train_accuracy = correct / len(train_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_accuracy:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run experiments\n",
        "print(\"Training with ReLU + Adam:\")\n",
        "model_relu_adam = train_and_evaluate('relu', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Tanh + Adam:\")\n",
        "model_tanh_adam = train_and_evaluate('tanh', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Sigmoid + SGD:\")\n",
        "model_sigmoid_sgd = train_and_evaluate('sigmoid', 'sgd')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results of the final iteration showed no improvement in accuracy or loss rates, with accuracy dropping to ~10% again and loss skyrocketing like the initial iteration. \n",
        "\n",
        "In conclusion, a learning rate of 0.1 is ideal to achieve high accuracy rate in the SGD optimiser when used with the Sigmoid activation function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we know the ideal learning rate value, in the next iteration I aim to check whether increasing the number of epochs from 10 to 15 has an impact on the loss and accuracy rate of all the three activation function + optimizer combinations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with ReLU + Adam:\n",
            "Epoch 1: Loss = 559.2046, Training Accuracy = 0.7804\n",
            "Epoch 2: Loss = 399.0117, Training Accuracy = 0.8467\n",
            "Epoch 3: Loss = 364.9941, Training Accuracy = 0.8603\n",
            "Epoch 4: Loss = 346.4972, Training Accuracy = 0.8666\n",
            "Epoch 5: Loss = 332.2067, Training Accuracy = 0.8716\n",
            "Epoch 6: Loss = 319.7782, Training Accuracy = 0.8764\n",
            "Epoch 7: Loss = 306.9475, Training Accuracy = 0.8815\n",
            "Epoch 8: Loss = 304.0997, Training Accuracy = 0.8827\n",
            "Epoch 9: Loss = 295.7075, Training Accuracy = 0.8851\n",
            "Epoch 10: Loss = 289.4848, Training Accuracy = 0.8877\n",
            "Epoch 11: Loss = 280.6306, Training Accuracy = 0.8899\n",
            "Epoch 12: Loss = 275.1522, Training Accuracy = 0.8907\n",
            "Epoch 13: Loss = 273.9605, Training Accuracy = 0.8921\n",
            "Epoch 14: Loss = 268.2923, Training Accuracy = 0.8952\n",
            "Epoch 15: Loss = 264.1560, Training Accuracy = 0.8959\n",
            "\n",
            "Training with Tanh + Adam:\n",
            "Epoch 1: Loss = 518.2301, Training Accuracy = 0.8017\n",
            "Epoch 2: Loss = 409.2020, Training Accuracy = 0.8430\n",
            "Epoch 3: Loss = 383.2794, Training Accuracy = 0.8525\n",
            "Epoch 4: Loss = 367.3007, Training Accuracy = 0.8592\n",
            "Epoch 5: Loss = 352.9305, Training Accuracy = 0.8640\n",
            "Epoch 6: Loss = 347.0410, Training Accuracy = 0.8669\n",
            "Epoch 7: Loss = 339.6297, Training Accuracy = 0.8692\n",
            "Epoch 8: Loss = 335.3089, Training Accuracy = 0.8712\n",
            "Epoch 9: Loss = 328.8739, Training Accuracy = 0.8724\n",
            "Epoch 10: Loss = 321.6909, Training Accuracy = 0.8742\n",
            "Epoch 11: Loss = 322.9859, Training Accuracy = 0.8759\n",
            "Epoch 12: Loss = 317.0527, Training Accuracy = 0.8778\n",
            "Epoch 13: Loss = 312.7531, Training Accuracy = 0.8782\n",
            "Epoch 14: Loss = 310.9285, Training Accuracy = 0.8779\n",
            "Epoch 15: Loss = 308.9558, Training Accuracy = 0.8798\n",
            "\n",
            "Training with Sigmoid + SGD:\n",
            "Epoch 1: Loss = 2174.7781, Training Accuracy = 0.0985\n",
            "Epoch 2: Loss = 2171.0229, Training Accuracy = 0.1002\n",
            "Epoch 3: Loss = 2168.7904, Training Accuracy = 0.1002\n",
            "Epoch 4: Loss = 2167.2936, Training Accuracy = 0.1007\n",
            "Epoch 5: Loss = 2165.4390, Training Accuracy = 0.1020\n",
            "Epoch 6: Loss = 2164.1188, Training Accuracy = 0.1030\n",
            "Epoch 7: Loss = 2163.3252, Training Accuracy = 0.1019\n",
            "Epoch 8: Loss = 2162.2099, Training Accuracy = 0.1031\n",
            "Epoch 9: Loss = 2162.0981, Training Accuracy = 0.1025\n",
            "Epoch 10: Loss = 2161.0056, Training Accuracy = 0.1044\n",
            "Epoch 11: Loss = 2160.2644, Training Accuracy = 0.1040\n",
            "Epoch 12: Loss = 2160.2596, Training Accuracy = 0.1053\n",
            "Epoch 13: Loss = 2159.2503, Training Accuracy = 0.1066\n",
            "Epoch 14: Loss = 2158.5373, Training Accuracy = 0.1075\n",
            "Epoch 15: Loss = 2157.7778, Training Accuracy = 0.1112\n"
          ]
        }
      ],
      "source": [
        "#Iteration 2 \n",
        "def train_and_evaluate(activation='relu', optimizer_type='adam', epochs=15):\n",
        "    model = NewFeedForwardNet()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Choose optimizer\n",
        "    if optimizer_type == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif optimizer_type == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer choice\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(images, activation)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(dim=1) == labels).sum().item()\n",
        "        \n",
        "        train_accuracy = correct / len(train_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Training Accuracy = {train_accuracy:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run experiments\n",
        "print(\"Training with ReLU + Adam:\")\n",
        "model_relu_adam = train_and_evaluate('relu', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Tanh + Adam:\")\n",
        "model_tanh_adam = train_and_evaluate('tanh', 'adam')\n",
        "\n",
        "print(\"\\nTraining with Sigmoid + SGD:\")\n",
        "model_sigmoid_sgd = train_and_evaluate('sigmoid', 'sgd')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results of increasing the epochs from 10 to 15 does not display the epochs for Sigmoid + SGD at all. The testing stops at the tanh+Adam stage. This could be due to various reasons such as the following but not limited to;\n",
        "1. Vanishing Gradient: When using sigmoid activation functions, gradients can become extremely small during backpropagation, especially in deeper networks. This is because the derivative of the sigmoid function ranges from 0 to 0.25, with very small values. This causes the network to learn very slowly or not at all.\n",
        "\n",
        "2. Learning Rate Sensitivity: SGD is particularly sensitive to learning rate when paired with sigmoid activations. As noted in one discussion, changing from Adam to SGD required significant learning rate adjustments, but even with various learning rate values, SGD still struggled to train effectively."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

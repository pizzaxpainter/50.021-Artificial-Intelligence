{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzKI3DMiYG7"
      },
      "source": [
        "# PyTorch Tutorial - Sequence Models\n",
        "\n",
        "In view of the AI quiz, capstone and other deliverables during week 5, the exercises for this lab is made relatively straightforward. The homework exercises can be found at the end of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp-0ZJqT8ehJ"
      },
      "source": [
        "## Sentiment detection with BERT\n",
        "\n",
        "The task tackled here is sentiment detection of IMDB articles. In this notebook, we will utilize the Huggingface suite of libraries (transformers, datasets, evaluate, etc) to build upon it to perform some quick prototyping and evaluation of models. Some of the concepts we will use:\n",
        "- loading pre-trained models\n",
        "- fine-tuning on downstream tasks\n",
        "- streamlining the training loop\n",
        "- evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8vt_mfgiYG8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnexpected end of JSON input. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Check torch version\n",
        "print(torch.__version__)\n",
        "# Check for CUDA\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-IdwM3Q9_HJ"
      },
      "source": [
        "First, we install and import various libraries that are useful for this task, namely:\n",
        "- datasets: Contains numerous popular datasets of different modalities and sizes, along with useful pre-processing tools (https://github.com/huggingface/datasets)\n",
        "- transformers: Contains numerous pre-trained model from the huggingface community, along with many useful pipelines for automating the training and evaluation process (https://github.com/huggingface/transformers)\n",
        "- evaluate: Contains numerous standard and custom evaluation metrics that can be easily used as part of the evaluation pipeline (https://github.com/huggingface/evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jHMSZlNPiYG_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (from evaluate) (1.5.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
            "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.1)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.19.0 (from evaluate)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kripashree/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, huggingface-hub, datasets, evaluate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "supervisely 6.73.212 requires pydantic<=2.8.2,>=1.7.4, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 huggingface-hub-0.29.2 multiprocess-0.70.16 requests-2.32.3 xxhash-3.5.0\n",
            "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.10/site-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (from datasets) (1.5.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/homebrew/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (0.29.2)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.10/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (1.15.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/kripashree/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /Users/kripashree/Library/Python/3.10/lib/python/site-packages (from accelerate) (5.9.2)\n",
            "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/lib/python3.10/site-packages (from accelerate) (0.29.2)\n",
            "Collecting safetensors>=0.4.3 (from accelerate)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
            "Installing collected packages: safetensors, accelerate\n",
            "Successfully installed accelerate-1.4.0 safetensors-0.5.3\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: regex, tokenizers, transformers\n",
            "Successfully installed regex-2024.11.6 tokenizers-0.21.0 transformers-4.49.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U evaluate\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import accelerate\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1m-ETbeG590"
      },
      "source": [
        "The evaluate library provides many standard evaluation metrics (such as f1, precision, accuracy, etc) for specific tasks. For example, the list of standard metrics for the text classification task can be found at https://huggingface.co/tasks/text-classification. Similarly, there are also many custom-made metrics for specific tasks, which the foillowing code prints out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HYHBk5CjiYG_",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['codeparrot/apps_metric', 'lvwerra/test', 'angelina-wang/directional_bias_amplification', 'cpllab/syntaxgym', 'lvwerra/bary_score', 'hack/test_metric', 'yzha/ctc_eval', 'mfumanelli/geometric_mean', 'daiyizheng/valid', 'erntkn/dice_coefficient', 'mgfrantz/roc_auc_macro', 'Vlasta/pr_auc', 'gorkaartola/metric_for_tp_fp_samples', 'idsedykh/metric', 'idsedykh/codebleu2', 'idsedykh/codebleu', 'idsedykh/megaglue', 'Vertaix/vendiscore', 'GMFTBY/dailydialogevaluate', 'GMFTBY/dailydialog_evaluate', 'jzm-mailchimp/joshs_second_test_metric', 'ola13/precision_at_k', 'yulong-me/yl_metric', 'abidlabs/mean_iou', 'abidlabs/mean_iou2', 'KevinSpaghetti/accuracyk', 'NimaBoscarino/weat', 'ronaldahmed/nwentfaithfulness', 'Viona/infolm', 'kyokote/my_metric2', 'kashif/mape', 'Ochiroo/rouge_mn', 'leslyarun/fbeta_score', 'anz2/iliauniiccocrevaluation', 'zbeloki/m2', 'xu1998hz/sescore', 'dvitel/codebleu', 'NCSOFT/harim_plus', 'JP-SystemsX/nDCG', 'sportlosos/sescore', 'Drunper/metrica_tesi', 'jpxkqx/peak_signal_to_noise_ratio', 'jpxkqx/signal_to_reconstruction_error', 'hpi-dhc/FairEval', 'lvwerra/accuracy_score', 'ybelkada/cocoevaluate', 'harshhpareek/bertscore', 'posicube/mean_reciprocal_rank', 'bstrai/classification_report', 'omidf/squad_precision_recall', 'Josh98/nl2bash_m', 'BucketHeadP65/confusion_matrix', 'BucketHeadP65/roc_curve', 'yonting/average_precision_score', 'transZ/test_parascore', 'transZ/sbert_cosine', 'hynky/sklearn_proxy', 'xu1998hz/sescore_english_mt', 'xu1998hz/sescore_german_mt', 'xu1998hz/sescore_english_coco', 'xu1998hz/sescore_english_webnlg', 'unnati/kendall_tau_distance', 'Viona/fuzzy_reordering', 'Viona/kendall_tau', 'lhy/hamming_loss', 'lhy/ranking_loss', 'Muennighoff/code_eval_octopack', 'yuyijiong/quad_match_score', 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics', 'Yeshwant123/mcc', 'phonemetransformers/segmentation_scores', 'sma2023/wil', 'chanelcolgate/average_precision', 'ckb/unigram', 'Felipehonorato/eer', 'manueldeprada/beer', 'shunzh/apps_metric', 'He-Xingwei/sari_metric', 'langdonholmes/cohen_weighted_kappa', 'fschlatt/ner_eval', 'hyperml/balanced_accuracy', 'brian920128/doc_retrieve_metrics', 'guydav/restrictedpython_code_eval', 'k4black/codebleu', 'Natooz/ece', 'ingyu/klue_mrc', 'Vipitis/shadermatch', 'gabeorlanski/bc_eval', 'jjkim0807/code_eval', 'repllabs/mean_reciprocal_rank', 'repllabs/mean_average_precision', 'mtc/fragments', 'DarrenChensformer/eval_keyphrase', 'kedudzic/charmatch', 'Vallp/ter', 'DarrenChensformer/relation_extraction', 'Ikala-allen/relation_extraction', 'danieldux/hierarchical_softmax_loss', 'nlpln/tst', 'bdsaglam/jer', 'davebulaval/meaningbert', 'fnvls/bleu1234', 'fnvls/bleu_1234', 'nevikw39/specificity', 'yqsong/execution_accuracy', 'shalakasatheesh/squad_v2', 'arthurvqin/pr_auc', 'd-matrix/dmx_perplexity', 'akki2825/accents_unplugged_eval', 'juliakaczor/accents_unplugged_eval', 'Vickyage/accents_unplugged_eval', 'Qui-nn/accents_unplugged_eval', 'TelEl/accents_unplugged_eval', 'livvie/accents_unplugged_eval', 'DaliaCaRo/accents_unplugged_eval', 'alvinasvk/accents_unplugged_eval', 'LottieW/accents_unplugged_eval', 'LuckiestOne/valid_efficiency_score', 'Fritz02/execution_accuracy', 'huanghuayu/multiclass_brier_score', 'jialinsong/apps_metric', 'DoctorSlimm/bangalore_score', 'agkphysics/ccc', 'DoctorSlimm/kaushiks_criteria', 'CZLC/rouge_raw', 'bascobasculino/mot-metrics', 'SEA-AI/mot-metrics', 'SEA-AI/det-metrics', 'saicharan2804/my_metric', 'red1bluelost/evaluate_genericify_cpp', 'maksymdolgikh/seqeval_with_fbeta', 'Bekhouche/NED', 'danieldux/isco_hierarchical_accuracy', 'ginic/phone_errors', 'berkatil/map', 'DarrenChensformer/action_generation', 'buelfhood/fbeta_score', 'danasone/ru_errant', 'helena-balabin/youden_index', 'SEA-AI/panoptic-quality', 'SEA-AI/box-metrics', 'MathewShen/bleu', 'berkatil/mrr', 'BridgeAI-Lab/SemF1', 'SEA-AI/horizon-metrics', 'maysonma/lingo_judge_metric', 'dannashao/span_metric', 'Aye10032/loss_metric', 'ag2435/my_metric', 'kilian-group/arxiv_score', 'bomjin/code_eval_octopack', 'svenwey/logmetric', 'bowdbeg/matching_series', 'BridgeAI-Lab/Sem-nCG', 'bowdbeg/patch_series', 'venkatasg/gleu', 'kbmlcoding/apps_metric', 'jijihuny/ecqa', 'prajwall/mse', 'd-matrix/dmxMetric', 'dotkaio/competition_math', 'bowdbeg/docred', 'Remeris/rouge_ru', 'jarod0411/aucpr', 'Ruchin/jaccard_similarity', 'phucdev/blanc_score', 'NathanMad/bertscore-with-torch_dtype', 'cointegrated/blaser_2_0_qe', 'ahnyeonchan/Alignment-and-Uniformity', 'Baleegh/Fluency_Score', 'mdocekal/multi_label_precision_recall_accuracy_fscore', 'phucdev/vihsd', 'argmaxinc/detailed-wer', 'SEA-AI/user-friendly-metrics', 'hage2000/code_eval_stdio', 'hage2000/my_metric', 'Natooz/levenshtein', 'Khaliq88/execution_accuracy', 'pico-lm/perplexity', 'mtzig/cross_entropy_loss', 'kiracurrie22/precision', 'openpecha/bleurt', 'SEA-AI/ref-metrics', 'Natooz/mse', 'buelfhood/fbeta_score_2', 'murinj/hter', 'ncoop57/levenshtein_distance', 'kaleidophon/almost_stochastic_order', 'NeuraFusionAI/Arabic-Evaluation', 'lvwerra/element_count', 'prb977/cooccurrence_count', 'NimaBoscarino/pseudo_perplexity', 'ybelkada/toxicity', 'ronaldahmed/ccl_win', 'christopher/tokens_per_byte', 'lsy641/distinct', 'grepLeigh/perplexity', 'Charles95/element_count', 'Charles95/accuracy', 'Lucky28/honest']\n"
          ]
        }
      ],
      "source": [
        "metrics_list = evaluate.list_evaluation_modules()\n",
        "print(metrics_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwB8B-1nI0Ye"
      },
      "source": [
        "We now load in the dataset of interest for this task, which is the IMDB dataset that contains a series of movie reviews and the corresponding sentiment label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "q-SVeAt-iYHA"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pt3ohCP4mrBE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D0tJ_S6Booo"
      },
      "source": [
        "##Transfer Learning\n",
        "\n",
        "Transfer learning is technique where we take a model that has learnt to solve one problem, then apply it to solving a different but related problem. In this tutorial, we are applying transfer learning by taking the pre-trained BERT-tiny model and fine-tuning it on this task of sentiment detection on IMDB. Details on the BERT-tiny model can be found at https://huggingface.co/google/bert_uncased_L-2_H-128_A-2.\n",
        "\n",
        "The following code loads a model checkpoint from huggingface (i.e., BERT-tiny as defined in the model_checkpoint variable), which you can use to further fine-tune on the specific downstream task (i.e., IMDB sentiment detection). Apart from a standard huggingface model checkpoint, you can also use this to load one of your previously trained model, e.g., to continue more training or transfer learn on another task. This code also performs some pre-processing of the dataset in terms of tokenization and padding/truncating the text to a specific sequence that BERT requires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GuxO_ZyviYHA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "541e233e7ca546388657bf19e3e48fef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/382 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6998ec1778d84888b316f17a98c62517",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b96b89b50c0a43bfbfbdd4ab5c3c1e7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0019c226f00040a1a9bbcb36eafcbf93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a364deb8c9b941a7b9dbfe924c274bbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "max_len = 512\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "\n",
        "def preprocess_function(input_data):\n",
        "    texts = input_data\n",
        "    return tokenizer(texts[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cBixVT5iiYHB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 25000\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tofQgqfuKwbZ"
      },
      "source": [
        "AutoModelForSequenceClassification provides a high-level interface/wrapper around specific pre-trained models that you can use for general text classification tasks, including finetuning it based on various configurations. More details can be found at https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KILhS03RiYHB"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25efc135986c442c8bd4508f8153ff57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Embedding(30522, 128, padding_idx=0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, hidden_dropout_prob=0.1)\n",
        "model.resize_token_embeddings(len(tokenizer)) # need to resize due to new tokens added"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EICviuNBRpgG"
      },
      "source": [
        "Here, TrainingArguments allow us to specific various parameters relating to the training of our model, including training epochs, batch sizes, etc. More details can be found at https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.TrainingArguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YHbw6fYjiYHB"
      },
      "outputs": [],
      "source": [
        "metric_name = 'f1'\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"./snapshots/{model_name}-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=False,\n",
        "    fp16=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3ig_RGPSl7K"
      },
      "source": [
        "For computation of our evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p9W2ujpyiYHC"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "477982e4fb044649b9c2bc823e988cf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "metric = evaluate.load(metric_name)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels, average=\"micro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8xaX0rRSxwD"
      },
      "source": [
        "Now, we initialize a Trainer that handles the training loop based on our definition of the model and training parameters as defined in the earlier part of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UuDfw3v_iYHC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/p0/6sw494x1303f8w2k35bxtgk80000gn/T/ipykernel_76527/3731775950.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['train'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxeojkzkTJTm"
      },
      "source": [
        "Start the actual model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1MXcGcMniYHC",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "source": [
        "train_log = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yog8tLG8iYHC"
      },
      "outputs": [],
      "source": [
        "# trainer.save_model(\"./models/myFinetunedModel\") # for saving your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgUw9z26TNYW"
      },
      "source": [
        "Finally, we perform our evaluation on our test set using the fine-tuned model from earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xlONu3IXOxZz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m classifier(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], max_length\u001b[38;5;241m=\u001b[39mmax_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m dfResults \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(results)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:85\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     88\u001b[0m         TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[1;32m     91\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/pipelines/base.py:985\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m ):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the model can generate:\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massistant_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massistant_tokenizer \u001b[38;5;241m=\u001b[39m load_assistant_model(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    993\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
        "results = classifier(dataset['test']['text'], max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "dfResults = pd.DataFrame.from_dict(results)\n",
        "dfResults['label'] = dfResults['label'].str.replace('LABEL_','')\n",
        "f1 = metric.compute(predictions=dfResults['label'].tolist(), references=dataset['test']['label'], average='micro')\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neCpaLh2TliU"
      },
      "source": [
        "## Homework Exercises\n",
        "**Due: 14th Mar, 11:59pm**\n",
        "<br>\n",
        "<br>\n",
        "Submit your homework as either: (i) an ipynb file with your results inside; or (ii) a python file and separate pdf discussing your results. Based on what you have done so far in this tutorial, repeat another set of experiments with the following changes:\n",
        "\n",
        "(a) There is a potential error in the design of the training and testing/evaluation step in this tutorial. Identify what it is and describe how you will solve this.\n",
        "\n",
        "(b) Pick another dataset from the datasets library that is not a binary classification task (see https://huggingface.co/datasets). You can sample a subset from this dataset to reduce compute time (see https://huggingface.co/docs/datasets/en/process).\n",
        "\n",
        "(c) Fine-tune the same model but with a dropout of 0.2, over 3 epochs and report on accuracy scores, in addition to F1 scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part a:\n",
        "\n",
        "The potential error in the design of the training and testing/ evaluation step is caused due to the same data samples being used for both training and evaluation:\n",
        "\n",
        "train_dataset=encoded_dataset['train'],\n",
        "eval_dataset=encoded_dataset['train'],\n",
        "\n",
        "To solve the issue, the evaluation step must use different dataset/ different samples as shown in the code below:\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "Since the tutorial does not contain a split dataset, I will proceed to split the dataset and implement the trainer method as shown below. I also need to change some parameters in the args method as I do not have an NVIDIA GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/var/folders/p0/6sw494x1303f8w2k35bxtgk80000gn/T/ipykernel_76527/2739572784.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [30], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(train_df)\n\u001b[1;32m     31\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(eval_df)\n\u001b[0;32m---> 33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
          ]
        }
      ],
      "source": [
        "#Part a\n",
        "#New Trainer Method\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"./snapshots/{model_name}-finetuned\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=False,\n",
        "    fp16=False  # Changed to False\n",
        ")\n",
        "\n",
        "train_data = encoded_dataset['train']\n",
        "train_df = train_data.to_pandas()\n",
        "train_df, eval_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6f2ef3f86914ff4959e5ffb3c0d3929",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7c87c344a8944bab02f39b612591f3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8410be6096ec47a0ab1f02d7e28bcb8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d8f7c98af84d9d9276b8fa6363c8c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load AG News dataset (multi-class classification)\n",
        "dataset_new = load_dataset(\"ag_news\")\n",
        "\n",
        "# Sample a subset to reduce compute time (e.g., 10% of the dataset)\n",
        "small_train_dataset = dataset_new[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "small_test_dataset = dataset_new[\"test\"].shuffle(seed=42).select(range(1000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "print(dataset_new[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True)  # Change \"text\" to \"content\" if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['text', 'label']\n"
          ]
        }
      ],
      "source": [
        "print(dataset_new[\"train\"].column_names)  # This will show all available column names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_name = \"content\"  # Change this based on the actual dataset structure\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[column_name], padding=\"max_length\", truncation=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b1de1f81df84f1ba1deff7b2360f69d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/opt/homebrew/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [83], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m encoded_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Using test set for validation\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     71\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load dataset (Choose a multi-class dataset instead of binary classification)\n",
        "dataset_new = load_dataset(\"ag_news\")  # Change this if needed\n",
        "\n",
        "# Check dataset structure\n",
        "print(dataset_new[\"train\"][0])  # Print first example to verify column names\n",
        "column_name = \"text\"  # Adjust based on dataset\n",
        "\n",
        "# Load tokenizer\n",
        "model_checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "max_len = 512\n",
        "\n",
        "# Tokenization function\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[column_name], max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply tokenization\n",
        "encoded_dataset = dataset_new.map(preprocess_function, batched=True)\n",
        "\n",
        "# Get number of unique labels\n",
        "num_labels = len(set(dataset_new[\"train\"][\"label\"]))\n",
        "\n",
        "# Load model with custom dropout\n",
        "config = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels, hidden_dropout_prob=0.2)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config)\n",
        "\n",
        "# Compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")  # Weighted for multi-class\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "# Training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=f\"./snapshots/{model_checkpoint}-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,  # Fine-tuning for 3 epochs\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "train_dataset = encoded_dataset[\"train\"]\n",
        "val_dataset = encoded_dataset[\"test\"]  # Using test set for validation\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Final validation accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Final validation F1 score: {eval_results['eval_f1']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
